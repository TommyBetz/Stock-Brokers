#!/usr/bin/env python
# coding: utf-8

# Import libraries
import pandas as pd
# Make sure you have newspaper installed
# newspaper: https://newspaper.readthedocs.io/en/latest/
from newspaper import Article 

# Split the datset into smaller dataset for reducing the Scraping load
data = pd.read_json('News_2012_2018.json',lines=True)
data = data[(data['date'] >= '2012-01-01') & (data['date'] < '2013-01-01')]
# Create different files from 2012 to 2018
data.to_csv('huff_2012.csv')

# For all files that are generated by the above code process those files in chunks of 100 so it reduces less memory buffer
data = pd.read_csv('huff_2018.csv',lines=True,chunksize = 100)
def get_body(url):
    try:
        split_word = 'http'
        url = url.split(split_word, 2)[-1]
        url = split_word + url
        article = Article(url)
        article.download()
        article.parse()
        if len(article.text)==0:
            return ''
        else:
            return article.text
    except:
        return ''

# For each file write each processed chunk to csv
for i,chunk in enumerate(data):
    chunk['body'] = chunk['link'].map(get_body)
    chunk.to_csv('news_2018/{}.csv'.format(i))
    print(i)